{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ccf9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb517033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sets(path):\n",
    "#     \"\"\"\n",
    "#     Parses the raw FB text files and returns the set of all entities(their machine ids) and relations\n",
    "    \n",
    "#     !!!The entities are sorted before assigning them to an index\n",
    "    \n",
    "#     Input: path object\n",
    "    \n",
    "#     Output: dictionaries of unique integer index to each entity and relation, set of edges for each split\n",
    "#     \"\"\"\n",
    "#     entities, relations = set(), set()\n",
    "#     edge_set = {}\n",
    "#     for split in [\"train.txt\", \"valid.txt\", \"test.txt\"]:\n",
    "#         with open(os.path.join(path, split), \"r\") as lines:\n",
    "#             edges = set()\n",
    "#             for line in lines:\n",
    "#                 lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "#                 entities.add(lhs)\n",
    "#                 entities.add(rhs)\n",
    "#                 relations.add(rel)\n",
    "#                 edges.add((lhs,rel,rhs))\n",
    "#         edge_set[split] = edges\n",
    "    \n",
    "#     ent_id = {k:i for (i,k) in enumerate(sorted(entities))}\n",
    "#     rel_id = {k:i for (i,k) in enumerate(sorted(relations))}\n",
    "                \n",
    "#     return ent_id, rel_id, edge_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a89349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(path, edge_filter = False, max_edges = None):\n",
    "    \"\"\"\n",
    "    Parses the raw FB text files and returns the set of all entities(their machine ids) and relations\n",
    "    \n",
    "    !!!The entities are sorted before assigning them to an index\n",
    "    \n",
    "    Input: path object\n",
    "    \n",
    "    Output: dictionaries of unique integer index to each entity and relation, set of edges for each split\n",
    "    \"\"\"\n",
    "    entities, relations = set(), set()\n",
    "    edge_set = {}\n",
    "    for split in [\"train.txt\", \"valid.txt\", \"test.txt\"]:\n",
    "        with open(os.path.join(path, split), \"r\") as lines:\n",
    "            edges = set()\n",
    "            for line in lines:\n",
    "                lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "                entities.add(lhs)\n",
    "                entities.add(rhs)\n",
    "                relations.add(rel)\n",
    "                edges.add((lhs,rel,rhs))\n",
    "        edge_set[split] = edges\n",
    "    \n",
    "    ent_id = {k:i for (i,k) in enumerate(sorted(entities))}\n",
    "    rel_id = {k:i for (i,k) in enumerate(sorted(relations))}\n",
    "    \n",
    "    if edge_filter is True:\n",
    "        assert max_edges != None, 'need to specify maximum number of edges when filtering'\n",
    "        connect_count = {k:0 for k in list(ent_id.keys())}\n",
    "        with open(os.path.join(path, 'train.txt'), \"r\") as lines:\n",
    "            for line in lines:\n",
    "                lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "                connect_count[lhs] = connect_count[lhs] + 1\n",
    "                connect_count[rhs] = connect_count[rhs] + 1\n",
    "\n",
    "#         id_to_keep = (np.where(connect_count <= max_edges)[0]).tolist() #indices of all entities with less than maximum edges\n",
    "#         keep_ent = [list(ent_id.keys())[i] for i in id_to_keep] #list of entities (mids) with less than max edges \n",
    "#         del_embeddings = [k for k in list(ent_id.keys()) if k not in keep_ent] #list of deleted entities\n",
    "        keep_ent = [k for k,v in connect_count.items() if v <= max_edges]\n",
    "        ent_id = {k:i for i,k in enumerate(keep_ent)}\n",
    "        edge_set = {}\n",
    "        for split in [\"train.txt\", \"valid.txt\", \"test.txt\"]:\n",
    "            with open(os.path.join(path, split), \"r\") as lines:\n",
    "                edges = set()\n",
    "                for line in lines:\n",
    "                    lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "                    if (lhs in keep_ent) and (rhs in keep_ent):\n",
    "                        edges.add((lhs,rel,rhs))\n",
    "            edge_set[split] = edges\n",
    "\n",
    "               \n",
    "    return ent_id, rel_id, edge_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fec570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/shazoop/KG-Embeddings/datasets/FB15K-237'\n",
    "# device = torch.device(1)\n",
    "\n",
    "# ent_id, rel_id, edge_set = get_sets(path, edge_filter = True, max_edges = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da51a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_filtered_embeddings(path, split, D_ent, D_rel, max_edges, rel_tensor = False):\n",
    "#     \"\"\"\n",
    "#     For each entity, generate the embedding of its neighborhood subgraph (all edges involving the entity).\n",
    "    \n",
    "#     input: path object, data split, entity embedding dim, relation embedding dim\n",
    "#     \"\"\"\n",
    "#     ent_id, rel_id, ent_code, rel_code, edge_set = get_codebook(path, D_ent, D_rel)\n",
    "#     N_ent = len(ent_id)\n",
    "\n",
    "#     #Remove entities with more edges than max_edges\n",
    "#     connect_count = np.zeros(N_ent)\n",
    "#     if rel_tensor == True:\n",
    "#         embeddings = np.zeros((N_ent,D_ent,D_ent,D_rel+1))\n",
    "#     else:\n",
    "#         embeddings = np.zeros((N_ent,D_ent,D_ent))\n",
    "    \n",
    "#     with open(os.path.join(path, split), \"r\") as lines:\n",
    "#         for line in lines:\n",
    "#             lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "#             head, head_ix = ent_code[lhs], ent_id[lhs]\n",
    "#             tail, tail_ix = ent_code[rhs], ent_id[rhs]\n",
    "#             relation = rel_code[rel]\n",
    "#             if rel_tensor == True:\n",
    "#                 tensor = np.einsum('i,j,k -> ijk',head,tail,relation)\n",
    "#             else:\n",
    "#                 tensor = np.einsum('i,j -> ij', head, tail)\n",
    "#             embeddings[head_ix] += tensor\n",
    "#             embeddings[tail_ix] += tensor\n",
    "#             connect_count[head_ix] += 1\n",
    "#             connect_count[tail_ix] += 1\n",
    "            \n",
    "#     id_to_keep = (np.where(connect_count <= max_edges)[0]).tolist() #indices of all entities with less than maximum edges\n",
    "#     embeddings = embeddings[id_to_keep]\n",
    "#     keep_ent = [list(ent_id.keys())[i] for i in id_to_keep] #list of entities (mids) with less than max edges \n",
    "#     del_embeddings = [k for k in list(ent_id.keys()) if k not in keep_ent] #list of deleted entities\n",
    "#     mod_ent_code = {k:v for k,v in ent_code.items() if k in keep_ent} \n",
    "#     return embeddings, ent_id, rel_id, mod_ent_code, rel_code, edge_set, del_embeddings, ent_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8cafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codebook(path, D_ent, D_rel, edge_filter = False, max_edges = None):\n",
    "    \"\"\"\n",
    "    Generates a codebook for the set of entities and relations by randomly sampling from the unit hyperspheres of dimension\n",
    "    D_ent and D_rel respectively. \n",
    "    \n",
    "    input: path object, entity embedding dimension, relation embedding dimension\n",
    "    \n",
    "    output: dictionary of embeddings for entity and relation set (machine ids are keys for entity), plus output from get_id\n",
    "    \"\"\"\n",
    "    ent_id, rel_id, edge_set = get_sets(path, edge_filter, max_edges)\n",
    "    \n",
    "    def normal_vec(dim):\n",
    "        vec = np.random.multivariate_normal(np.zeros(dim),np.eye(dim))\n",
    "        return vec/np.linalg.norm(vec)\n",
    "    \n",
    "    def nv_append1(dim):\n",
    "        vec = np.random.multivariate_normal(np.zeros(dim),np.eye(dim))\n",
    "        return np.append(1.,vec/np.linalg.norm(vec))\n",
    "\n",
    "    \n",
    "    ent_code = {k:normal_vec(D_ent) for k in list(ent_id.keys())}\n",
    "    rel_code = {k:nv_append1(D_rel) for k in list(rel_id.keys())}\n",
    "    \n",
    "    return ent_id, rel_id, ent_code, rel_code, edge_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41963df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ent_id, rel_id, ent_code, rel_code, edge_set = get_codebook(path, 20, 20, edge_filter = True, max_edges = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2f4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(split, ent_id, rel_id, ent_code, rel_code, edge_set, rel_tensor = False):\n",
    "    \"\"\"\n",
    "    For each entity, generate the embedding of its neighborhood subgraph (all edges involving the entity).\n",
    "    \n",
    "    input: path object, data split, entity embedding dim, relation embedding dim\n",
    "    \"\"\"\n",
    "    N_ent = len(ent_id)\n",
    "    D_ent = (list(ent_code.values())[0]).shape[0]\n",
    "    D_rel = (list(rel_code.values())[0]).shape[0]\n",
    "    \n",
    "    if rel_tensor == True:\n",
    "        embeddings = np.zeros((N_ent,D_ent,D_ent,D_rel+1))\n",
    "    else:\n",
    "        embeddings = np.zeros((N_ent,D_ent,D_ent))\n",
    "    \n",
    "    for e in list(edge_set[split]):\n",
    "        head, rel, tail = e[0], e[1], e[2]\n",
    "        head_ebd, rel_ebd, tail_ebd = ent_code[head], rel_code[rel], ent_code[tail]\n",
    "        if rel_tensor == True:\n",
    "            tensor = np.einsum('i,j,k -> ijk',head_ebd,tail_ebd,rel_ebd)\n",
    "        else:\n",
    "            tensor = np.einsum('i,j -> ij', head_ebd,tail_ebd)\n",
    "        embeddings[ent_id[head]] += tensor\n",
    "        embeddings[ent_id[tail]] += tensor\n",
    "                \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbdaf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = get_embeddings('train.txt', ent_id, rel_id, ent_code, rel_code, edge_set, rel_tensor = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d35be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_embeddings(path, split, D_ent, D_rel, rel_tensor = False, edge_filter = False, max_edges = None):\n",
    "    \"\"\"\n",
    "    For each entity, generate the embedding of its neighborhood subgraph (all edges involving the entity).\n",
    "    \n",
    "    input: path object, data split, entity embedding dim, relation embedding dim\n",
    "    \"\"\"\n",
    "    ent_id, rel_id, ent_code, rel_code, edge_set = get_codebook(path, D_ent, D_rel, edge_filter, max_edges)\n",
    "    \n",
    "    embeddings =   get_embeddings(split, ent_id, rel_id, ent_code, rel_code, edge_set, rel_tensor)\n",
    "    \n",
    "    return embeddings, ent_id, rel_id, ent_code, rel_code, edge_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb37df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embeddings(path, split, D_ent, D_rel, rel_tensor = False, edge_filter = False, max_edges = None):\n",
    "#     \"\"\"\n",
    "#     For each entity, generate the embedding of its neighborhood subgraph (all edges involving the entity).\n",
    "    \n",
    "#     input: path object, data split, entity embedding dim, relation embedding dim\n",
    "#     \"\"\"\n",
    "#     ent_id, rel_id, ent_code, rel_code, edge_set = get_codebook(path, D_ent, D_rel, edge_filter, max_edges)\n",
    "#     N_ent = len(ent_id)\n",
    "    \n",
    "#     if rel_tensor == True:\n",
    "#         embeddings = np.zeros((N_ent,D_ent,D_ent,D_rel+1))\n",
    "#     else:\n",
    "#         embeddings = np.zeros((N_ent,D_ent,D_ent))\n",
    "    \n",
    "    \n",
    "#     for e in list(edge_set[split]):\n",
    "#         head, rel, tail = e[0], e[1], e[2]\n",
    "#         head_ebd, rel_ebd, tail_ebd = ent_code[head], rel_code[rel], ent_code[tail]\n",
    "#         if rel_tensor == True:\n",
    "#             tensor = np.einsum('i,j,k -> ijk',head_ebd,tail_ebd,rel_ebd)\n",
    "#         else:\n",
    "#             tensor = np.einsum('i,j -> ij', head_ebd,tail_ebd)\n",
    "#         embeddings[ent_id[head]] += tensor\n",
    "#         embeddings[ent_id[rel]] += tensor\n",
    "                \n",
    "#     return embeddings, ent_id, rel_id, ent_code, rel_code, edge_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b84720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embeddings(path, split, D_ent, D_rel, rel_tensor = False):\n",
    "#     \"\"\"\n",
    "#     For each entity, generate the embedding of its neighborhood subgraph (all edges involving the entity).\n",
    "    \n",
    "#     input: path object, data split, entity embedding dim, relation embedding dim\n",
    "#     \"\"\"\n",
    "#     ent_id, rel_id, ent_code, rel_code, edge_set = get_codebook(path, D_ent, D_rel)\n",
    "#     N_ent = len(ent_id)\n",
    "    \n",
    "#     if rel_tensor == True:\n",
    "#         embeddings = np.zeros((N_ent,D_ent,D_ent,D_rel+1))\n",
    "#     else:\n",
    "#         embeddings = np.zeros((N_ent,D_ent,D_ent))\n",
    "    \n",
    "#     with open(os.path.join(path, split), \"r\") as lines:\n",
    "#         for line in lines:\n",
    "#             lhs, rel, rhs = line.strip().split(\"\\t\")\n",
    "#             head = ent_code[lhs]\n",
    "#             tail = ent_code[rhs]\n",
    "#             relation = rel_code[rel]\n",
    "#             if rel_tensor == True:\n",
    "#                 tensor = np.einsum('i,j,k -> ijk',head,tail,relation)\n",
    "#             else:\n",
    "#                 tensor = np.einsum('i,j -> ij', head, tail)\n",
    "#             embeddings[ent_id[lhs]] += tensor\n",
    "#             embeddings[ent_id[rhs]] += tensor\n",
    "                \n",
    "#     return embeddings, ent_id, rel_id, ent_code, rel_code, edge_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73fdcc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_ePn(edge_set):\n",
    "    '''\n",
    "    Estimates average edges per node by just dividing number of edges by number of nodes\n",
    "    Input is the 'edge_set' output from any of the \"get_...\" functions\n",
    "    '''\n",
    "    edge_total = 0\n",
    "    for key in list(edge_set.keys()):\n",
    "        edge_total = edge_total + len(edge_set[key])\n",
    "    return(math.ceil(edge_total/len(ent_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86eec276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_to_device(nbd_embeddings, ent_code, rel_code, device):\n",
    "    ent_embeddings = torch.from_numpy(np.stack(list(ent_code.values()),0))\n",
    "    rel_embeddings = torch.from_numpy(np.stack(list(rel_code.values()),0))\n",
    "    ent_embeddings = ent_embeddings.to(device)\n",
    "    rel_embeddings = rel_embeddings.to(device)\n",
    "    nbd_embeddings = torch.from_numpy(nbd_embeddings).to(device) \n",
    "    return nbd_embeddings, ent_embeddings, rel_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1569d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tch_projmatrix(ent, ent_embeddings):\n",
    "    '''\n",
    "    Given the code for an entity h, will return a batch of projection matrices for each entity e.\n",
    "    If h,e are vector embeddings, then will compute eh^T for each entity e. Returns the batch of these matrices.\n",
    "    \n",
    "    Input: ent (vector embedding), dictionary of entity vector embeddings\n",
    "    \n",
    "    Output: batch of projection matrices, one for each entity\n",
    "    '''\n",
    "    return torch.einsum('ni,j ->nij',ent_embeddings,ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06d5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tch_score(edge, ent_embed, ttl_ent_embed , rel_embeddings, nbd_embeddings, ent_id, rel_id, k, rel_tensor= False):\n",
    "#     '''\n",
    "#     Score the proposed relation (h,r,t) by using similarity matching. Assume tail is the variable.\n",
    "#     we just use connectivity here and ignore relations when computing similarity\n",
    "#     edges is a tuple (mid1, relation, mid2)\n",
    "#     '''\n",
    "#     #Get the ids/vector embeddings for head, tail, relation. Get nbd embedding for head\n",
    "#     head, relation ,tail = edge[0], edge[1], edge[2]\n",
    "#     head_ix, tail_ix, relation_ix = ent_id[head], ent_id[tail], rel_id[relation]\n",
    "#     head_embed, tail_embed, rel_embed = ttl_ent_embed[head_ix], ttl_ent_embed[tail_ix], rel_embeddings[relation_ix]\n",
    "    \n",
    "#     if rel_tensor == True:\n",
    "#         head_nbd = nbd_embeddings[head_ix,:,:,0] #just use edge connectivity\n",
    "#         nbd_embed_norel = nbd_embeddings[:,:,:,0]\n",
    "#     else:\n",
    "#         head_nbd = nbd_embeddings[head_ix]\n",
    "#         nbd_embed_norel = nbd_embeddings\n",
    "    \n",
    "#     #Generate the projection matrices for the given head\n",
    "#     proj = tch_projmatrix(head_embed, ent_embed)\n",
    "#     Frob_norm = torch.norm(head_nbd)**2\n",
    "#     #Generate the \n",
    "    \n",
    "#     #Compute the graph homomorphism coeff\n",
    "#     x1 = torch.einsum('nij,jk -> nik',proj,head_nbd) #left multiply by projection matrix\n",
    "#     x2 = torch.einsum('ij,nkj -> nik',head_nbd,proj) #right multiply by transpose\n",
    "#     x = x1 + x2\n",
    "#     res = torch.bmm(x.permute(0,2,1),nbd_embed_norel)\n",
    "#     coeff = (1/Frob_norm)*torch.einsum('nii -> n',res) #number of matching edges\n",
    "    \n",
    "# #     #Get the top k\n",
    "# #     (val,ix) = torch.topk(coeff,k)\n",
    "# #     edge_coeff = torch.zeros(k-1).to(device)\n",
    "# #     for i in range(1,k): #ignore top match, since that's likely to be nbd_embedding of head itself\n",
    "# #         curr_ix = ix[i].cpu().item()\n",
    "# #         curr_head_embed = ent_embeddings[curr_ix]\n",
    "# #         edge_coeff[i-1] = torch.einsum('i,ij,j->',curr_head_embed,nbd_embeddings[curr_ix],tail_embed).cpu().item()\n",
    "    \n",
    "# #     #Compute score by weighting each score (-1,1) by softmax of the similarities\n",
    "# #     score = torch.dot(torch.nn.functional.softmax(val[1:],dim=0).float(),edge_coeff)\n",
    "\n",
    "#     #Get the top k\n",
    "#     (val,ix) = torch.topk(coeff,k)\n",
    "#     edge_coeff = torch.zeros(k).to(device)\n",
    "#     for i in range(k): #ignore top match, since that's likely to be nbd_embedding of head itself\n",
    "#         curr_ix = ix[i].cpu().item()\n",
    "#         curr_head_embed = ent_embeddings[curr_ix]\n",
    "#         edge_coeff[i] = torch.einsum('i,ij,j->',curr_head_embed,nbd_embeddings[curr_ix],tail_embed).cpu().item()\n",
    "#         edge_coeff = 2*edge_coeff.clamp(min=0.,max=1.) - 1\n",
    "#     #Compute score by weighting each score (-1,1) by softmax of the similarities\n",
    "#     score = torch.dot(val.float(),edge_coeff)\n",
    "\n",
    "    \n",
    "#     return score, (val,ix), edge_coeff\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45274a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tch_score(edge,ent_embeddings, rel_embeddings, nbd_embeddings, ent_id, rel_id, k, rel_tensor= False):\n",
    "    '''\n",
    "    Score the proposed relation (h,r,t) by using similarity matching. Assume tail is the variable.\n",
    "    we just use connectivity here and ignore relations when computing similarity\n",
    "    edges is a tuple (mid1, relation, mid2)\n",
    "    '''\n",
    "    #Get the ids/vector embeddings for head, tail, relation. Get nbd embedding for head\n",
    "    head, relation ,tail = edge[0], edge[1], edge[2]\n",
    "    head_ix, tail_ix, relation_ix = ent_id[head], ent_id[tail], rel_id[relation]\n",
    "    head_embed, tail_embed, rel_embed = ent_embeddings[head_ix], ent_embeddings[tail_ix], rel_embeddings[relation_ix]\n",
    "    \n",
    "    if rel_tensor == True:\n",
    "        head_nbd = nbd_embeddings[head_ix,:,:,0] #just use edge connectivity\n",
    "        nbd_embed_norel = nbd_embeddings[:,:,:,0]\n",
    "    else:\n",
    "        head_nbd = nbd_embeddings[head_ix]\n",
    "        nbd_embed_norel = nbd_embeddings\n",
    "    \n",
    "    #Generate the projection matrices for the given head\n",
    "    proj = tch_projmatrix(head_embed, ent_embeddings)\n",
    "    Frob_norm_sq = torch.norm(head_nbd)**2\n",
    "    #Generate the \n",
    "    \n",
    "    #Compute the graph homomorphism coeff\n",
    "    x1 = torch.einsum('nij,jk -> nik',proj,head_nbd) #left multiply by projection matrix\n",
    "    x2 = torch.einsum('ij,nkj -> nik',head_nbd,proj) #right multiply by transpose projection\n",
    "    x = x1 + x2\n",
    "    res = torch.bmm(x.permute(0,2,1),nbd_embed_norel)\n",
    "    coeff = (1/(Frob_norm_sq + 1e-3))*torch.einsum('nii -> n',res) #number of matching edges\n",
    "    \n",
    "    #Get the top k\n",
    "    (val,ix) = torch.topk(coeff,k)\n",
    "    edge_coeff = torch.zeros(k-1).to(device)\n",
    "    for i in range(1,k): #ignore top match, since that's likely to be nbd_embedding of head itself\n",
    "        curr_ix = ix[i].cpu().item()\n",
    "        edge_coeff[i-1] = torch.einsum('i,ij,j->',ent_embeddings[curr_ix],nbd_embeddings[curr_ix],tail_embed).cpu().item()\n",
    "#         edge_coeff = 2*edge_coeff.clamp(min=0.,max=1.) - 1\n",
    "    \n",
    "    #Compute score by weighting each score (-1,1) by softmax of the similarities\n",
    "    score = torch.dot(val[1:].float(),edge_coeff)\n",
    "    \n",
    "    return score, (val,ix), Frob_norm_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bab1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_batch, ent_embeddings, rel_embeddings, nbd_embeddings, ent_id, rel_id, k, edge_set, rel_tensor= False):\n",
    "    edges = list(edge_set['test.txt'])\n",
    "    batch = random.sample(edges,num_batch)\n",
    "    ttl_score = 0\n",
    "    num_edges = 0\n",
    "    \n",
    "    for e in batch:\n",
    "        score, _ , Frob_norm_sq= tch_score(e,ent_embeddings, rel_embeddings, nbd_embeddings, ent_id, rel_id, k, rel_tensor)\n",
    "        if Frob_norm_sq <= 1e-3:\n",
    "            continue\n",
    "        ttl_score = ttl_score + score.cpu().item()\n",
    "        num_edges = num_edges + 1\n",
    "    \n",
    "    return ttl_score/num_edges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73d0d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/shazoop/KG-Embeddings/datasets/FB15K-237'\n",
    "device = torch.device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a4c032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, ent_id, rel_id, ent_code, rel_code, edge_set = easy_embeddings(path, 'train.txt', D_ent = 80, D_rel = 80, rel_tensor = False, edge_filter = True, max_edges = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42271994",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbd_embeddings, ent_embeddings, rel_embeddings = embed_to_device(embeddings, ent_code, rel_code, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec9b7adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7298"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a8e5e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a92125ad4536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0medge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'edge_set' is not defined"
     ]
    }
   ],
   "source": [
    "edge = list(edge_set['test.txt'])[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea102cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-45ef87f0b914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtch_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ment_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbd_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_tensor\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'edge' is not defined"
     ]
    }
   ],
   "source": [
    "tch_score(edge,ent_embeddings, rel_embeddings, nbd_embeddings, ent_id, rel_id, 10, rel_tensor= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fa41c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11828963966609685"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(500, ent_embeddings, rel_embeddings, nbd_embeddings, ent_id, rel_id, 10, edge_set, rel_tensor= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47971164",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_ent_embed = ttl_ent_embeddings\n",
    "head, relation ,tail = edge[0], edge[1], edge[2]\n",
    "head_ix, tail_ix, relation_ix = ent_id[head], ent_id[tail], rel_id[relation]\n",
    "head_embed, tail_embed, rel_embed = ttl_ent_embed[head_ix], ttl_ent_embed[tail_ix], rel_embeddings[relation_ix]\n",
    "\n",
    "if rel_tensor == True:\n",
    "    head_nbd = nbd_embeddings[head_ix,:,:,0] #just use edge connectivity\n",
    "    nbd_embed_norel = nbd_embeddings[:,:,:,0]\n",
    "else:\n",
    "    head_nbd = nbd_embeddings[head_ix]\n",
    "    nbd_embed_norel = nbd_embeddings\n",
    "\n",
    "#Generate the projection matrices for the given head\n",
    "proj = tch_projmatrix(head_embed, ent_embed)\n",
    "Frob_norm = torch.norm(head_nbd)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3af3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbd_graph = nbd_embeddings[ent_id[edge[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a75b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(nbd_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dba72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_embed = ent_embeddings[0]\n",
    "tail_embed = ent_embeddings[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc90521",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = tch_projmatrix(head_embed, ent_embeddings)\n",
    "head_nbd = nbd_embeddings[0]\n",
    "Frob_norm = torch.norm(head_nbd)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b344a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj1 = torch.einsum('i,j -> ij', head_embed, head_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbdbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(torch.mm(proj1,head_nbd),proj1.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b763d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(head_nbd-x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_nbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ce98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.einsum('nij,jk -> nik',proj,head_nbd) #left multiply by projection matrix\n",
    "x = torch.einsum('nij,nkj -> nik',x,proj) #right multiply by transpose\n",
    "res = torch.bmm(x.permute(0,2,1),nbd_embeddings)\n",
    "coeff = (1/Frob_norm)*torch.einsum('nii -> n',res) #number of matching edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "(val,ix) = torch.topk(coeff,k)\n",
    "edge_coeff = torch.zeros(k-1).to(device)\n",
    "for i in range(1,k): #ignore top match, since that's likely to be nbd_embedding of head itself\n",
    "    curr_ix = ix[i].cpu().item()\n",
    "    edge_coeff[i-1] = torch.einsum('i,ij,j->',head_embed,nbd_embeddings[curr_ix],tail_embed).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "(val,ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = list(sorted(edge_set['train.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_id = list(ent_id.keys())[5625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(nbd_embeddings[5625])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483aa9e1",
   "metadata": {},
   "source": [
    "# Testing Graph Reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_embed = ent_embeddings[0]\n",
    "tail_embed = ent_embeddings[10:30]\n",
    "new_embed = ent_embeddings[31:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbd_test = torch.einsum('i,nj -> ij',head_embed,tail_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e75693",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(nbd_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008544cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('i,ij,nj ->n',head_embed,nbd_test,tail_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff89bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum('i,ij,nj ->n',head_embed,nbd_test,new_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(torch.mm(nbd_test.t(),nbd_test))/torch.norm(nbd_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_mat = torch.einsum('i,j -> ij',head_embed, head_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9009a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = torch.einsum('ij,jk -> ik',proj_mat,nbd_test) #left multiply by projection matrix\n",
    "x_test2 = torch.einsum('ij,kj -> ik', nbd_test,proj_mat) #right multiply by transpose\n",
    "x_test = x_test1 + x_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef533c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = torch.mm(x_test.t(),nbd_test)\n",
    "coeff = (1/(torch.norm(nbd_test)**2))*torch.einsum('ii -> ',res_test) #number of matching edges\n",
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed7c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
